# CSC 519 Project

## Milestone 1
For information about Milestone 1, see our [README](https://github.ncsu.edu/cscdevops-spring2021/DEVOPS-28/blob/M1/README.md) for that milestone.

## Milestone 2
Issue board for this milestone: https://github.ncsu.edu/cscdevops-spring2021/DEVOPS-28/projects/2

[March 29th checkpoint report](CHECKPOINT.md)

### Screencast

TODO

### Instructions for running the code

1. Clone the git repository locally.
2. In the project directory, run `npm install`
3. In the project directory, run `npm link`
4. Create a file in the project directory named ".vault-pass".  Inside the file, store the password for the ansible vault.
5. In the project directory, run the command `pipeline setup`.
6. In the project directory, run the command `pipeline build checkbox.io`.  Optionally, you may specify a username and password for Jenkins with the command `pipeline build checkbox.io -u <jenkins-user-id> -p <jenkins-password>`

### Automatically configure a build environment and build job for iTrust (thwinter)

### Implement a test suite analysis for detecting useful tests (anmcgill)

For this task, we defined a [driver.js](lib/driver.js) script that is run on the VM. This script uses a [mutate.js](lib/mutate.js) file to generate mutations.

The useful-tests command we defined first clones the iTrust repo and copies an application.yml file into the cloned project to configure the tests. The application.yml file uses a MYSQL_PASSWORD environment variable that is defined in our "environment" ansible role when MySQL is initially installed and configured. The command then calls the driver.js script, passing in the specified number of iterations.

The driver.js script extends the fuzzing workshop code to randomly select a file for each iteration, apply mutations to that file, attempt to compile the project using `mvn compile`, and then run tests using `mvn clean test`. It extends the test suite analysis code to parse the resulting XML files into a data structure and track the results for each test case over time. At the end of each iteration, the script drops the database generated by iTrust and resets the state of the mutated file.

The driver.js script has retry logic that handles these two cases:

1. The project failed to compile with the generated mutations. In this case, we discard the changes made to the file and try to select another file and mutate it.
2. A file was selected but mutations could not be generated. This could happen with several files in the iTrust project. For example, there's not much you can do with the repository interfaces. In this case, we just select another file and try to apply mutations to it.

We were asked to select two mutations in addition to those listed in the assignment, and these were the mutations we ultimately went with:

1. Replace && with ||
2. Replace return statements with `return null;`. For example, `return someValue;` or `return object.someMethod();` could both be replaced with `return null;`.

We initially had a different second mutation, but we found that we were hitting a lot of files where we couldn't mutate anything. The return statements mutation could be applied to most files, which is one of the reasons we decided to use it.

As the mutations are generated, they're written out to a /home/vagrant/mutations directory on the VM. The files within this directory are referenced from the test report.

The full output from running with 1,000 iterations is [here](usefulTests.txt). If you want to just see the report itself without all that debug output, that's also available in a [separate file](usefulTestsReport.txt).

The test suite found about 60% of the mutations, and the testDiagnoses test case within the APIDiagnosisTest class was the most effective at catching mutations:

```
Overall mutation coverage: 601/1000 (60.099999999999994%) mutations caught by the test suite.

Useful tests
============
271/1000 edu.ncsu.csc.iTrust2.api.APIDiagnosisTest.testDiagnoses
...
```

### Implement a static analysis for checkbox.io (sawalter)

For this task, we created a script, [analysis.js](/lib/analysis.js), based on the workshop code to run static analysis on the checkbox.io Jenkins job.  These tests include checking for long methods (greater than 100 lines of code), long message chains (more than 10 chained . commands after the initial reference), and excessive nesting depth (more than 5 deep).  This script is executed on every .js file in the checkbox.io repository, excluding those contained in the node_modules directory.  

Originally, we executed a shell command in the jenkins job script using find... exec to execute the script on these files.  Per the requirements, we need to fail the Jenkins build job when any of these conditions are violated.  To accomplish this, we decided to set a non-zero exit code of 1 any time one of these violation are detected.  We could then add up the exit code values from the analysis on each file, ultimately obtaining a calculation of the number of files containing violations, and if this number was greater than 1, could fail the build.  A limitation of using find... exec, however, was that the individual exit codes from each run of the analysis.js script were not retained.  In order to retain the exit codes, we instead decided to use a bash script, [staticAnalysis.sh](/cm/build-scripts/jjb-jobs/staticAnalysis.sh), implementing a for loop to cycle through each .js file, and storing the exit code after each run.  We then have the script return an exit code equal to the sum of the failures.  This script is run using the shell command in the Jenkins job.  Any time Jenkins receives a non-zero exit code, the build is automatically failed and immediately terminates.  

In order to continue Jenkins running the 'Test' stage even after a failure in the Static Analysis stage, we used the newly implemented catchError() command in Jenkins, which allowed us to specify that we wanted to fail the Static Analysis stage as well as failing the build, while not immediately terminating the build, allowing the later stages to run.  The screenshot below shows the final state of the build, with the Source, Build, and Test stages passing in green, and the Static Analysis stage failing in red.  The red ball indicates that the entire build failed as a result of the failing stage.


![Jenkins Build State](/screenshots/StaticAnalysis.png)


The Jenkins build log is also shown below, depicting both methods that passed the static analysis tests, as well as the errors for the three different types of failures.


![Build Failure Log](/screenshots/StaticAnalysis2.png)
